{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook trains different models on input data and and saves SWAG model in checkpoint directory to be used. This model is\\nthen used in calibration notebook to generate entropy\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This notebook trains different models on input data and and saves SWAG model in checkpoint directory to be used. This model is\n",
    "then used in calibration notebook to generate entropy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import tabulate\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52066,
     "status": "ok",
     "timestamp": 1649889219209,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "hSSHqV91w-Oq",
    "outputId": "d16cb7db-c8c1-48a1-9e3d-c3194a6560a6"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from swag import data, models, utils, losses\n",
    "from swag.posteriors import SWAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import imageio\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# import skimage.io as io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dApuGOQMSxG"
   },
   "source": [
    "## Model init:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initializing model, We can either use custom defined model or predefined models in the folder swag/models, in this case\n",
    "we are using PreResNet56 model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1649866418460,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "eJ4aNWliGgDI",
    "outputId": "b28e0854-4ed2-4d22-cf07-4c20c2c1dc94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing directory './checkpoints'\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# SWAG Checkpoints are saved in the ./checkpoints directory\n",
    "print(r\"Preparing directory './checkpoints'\")\n",
    "os.makedirs(r'./checkpoints', exist_ok=True)\n",
    "with open(os.path.join(r'./checkpoints', \"command.sh\"), \"w\") as f:\n",
    "    f.write(\" \".join(sys.argv))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "seed=1\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model can be defined here. Change linear layer depening on the size of input image used.\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 3,128,128\n",
    "        self.fc1 = nn.Linear(8192, 4096)\n",
    "\n",
    "#         self.fc1 = nn.Linear(2048, 4096)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(262144, 4096)\n",
    "#         self.fc1 = nn.Linear(32768, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5_1(x))\n",
    "        x = F.relu(self.conv5_2(x))\n",
    "        x = F.relu(self.conv5_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        print(x.shape)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5) #dropout was included to combat overfitting\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class ConvNetSimple:\n",
    "    base = VGG16\n",
    "    args = list()\n",
    "    kwargs = {}\n",
    "\n",
    "    transform_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(128),\n",
    "            # transforms.Resize(224),\n",
    "\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            transforms.Normalize((0.5394, 0.5354, 0.5504), (0.3623, 0.3620, 0.3465)),\n",
    "\n",
    "            # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            # transforms.Normalize((0.45242316, 0.45249584, 0.46897713), (0.21943445, 0.22656967, 0.22850613))\n",
    "        ]\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "    # transform_train = transforms.Compose(\n",
    "    #     [\n",
    "    #         transforms.RandomHorizontalFlip(),\n",
    "    #         transforms.Resize(256),\n",
    "    #         transforms.RandomCrop(256, padding=4),\n",
    "    #         transforms.Normalize((0.5394, 0.5354, 0.5504), (0.3623, 0.3620, 0.3465)),\n",
    "    #         # transforms.Normalize((0.4376821 , 0.4437697 , 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
    "    #     ]\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cfg  = ConvNetSimple\n",
    "# print(\"Preparing model\")\n",
    "# model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the custom model for parameters\n",
    "# summary(model, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9237,
     "status": "ok",
     "timestamp": 1649866427687,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "eABlJBE4Ifre",
    "outputId": "6362d965-46cd-4b49-ea00-aac002eec202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model\n",
      "<class 'swag.models.preresnet.Bottleneck'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg = getattr(models, 'PreResNet56')\n",
    "# model_cfg = getattr(models, 'VGG16')\n",
    "\n",
    "print(\"Preparing model\")\n",
    "# print(*model_cfg.args)\n",
    "model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQmdLOpRLmxb"
   },
   "source": [
    "### Dataset/Dataloader Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1649889632702,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "k1vBcD8nKrho"
   },
   "outputs": [],
   "source": [
    "def load_pngs():\n",
    "    good, bad = [], []\n",
    "    for im_path in glob.glob(\"./data/bottle/good/*.png\"):\n",
    "        im = Image.open(im_path)\n",
    "        good.append(im)\n",
    "    for im_path in glob.glob(\"./data/toothbrush/train/good/*.png\"):\n",
    "        im = Image.open(im_path)\n",
    "        bad.append(im)\n",
    "    return good, bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1649889637056,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "2o9qTHSOPs9U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_imgs, bad_imgs = load_pngs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1649889796762,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "7ygBH775FwTC"
   },
   "outputs": [],
   "source": [
    "# p=transforms.Compose(\n",
    "#     [transforms.Resize(size=128)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649889819912,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "s0fbuPQyD3mF",
    "outputId": "748743a7-7116-4861-d7b9-04c172ef7000"
   },
   "outputs": [],
   "source": [
    "# p(good_imgs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [i for i in good_imgs] +[i for i in bad_imgs]\n",
    "labels = [1 for i in good_imgs] +[0 for i in bad_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1649866461222,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "LvYztqXMOil7"
   },
   "outputs": [],
   "source": [
    "# images = []\n",
    "# labels = []\n",
    "# for i in good_imgs:\n",
    "#     images.append(i)\n",
    "#     labels += [1] \n",
    "\n",
    "# for i in bad_imgs:\n",
    "#     images.append(i)\n",
    "#     labels += [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1649866461222,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "wL2sRRn7NE1q",
    "outputId": "f6bf0b6b-a239-4981-921c-a677ee380499"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 151)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1649811130690,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "Zn8Vt6c4q2JE"
   },
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1649811131320,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "bbUYiTtfp-vM"
   },
   "outputs": [],
   "source": [
    "# img_tr = transform(images[0])\n",
    "\n",
    "# mean, std = img_tr.mean([1,2]), img_tr.std([1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1649811131320,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "9Zl3cOZBRMlv"
   },
   "outputs": [],
   "source": [
    "# print(labels[30])\n",
    "# p(images[30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1649866461223,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "YkhVs7fnzzCG"
   },
   "outputs": [],
   "source": [
    "class MvTecDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, imgs, labels, transform):\n",
    "        # self.imgs = imgs.astype(np.float32)\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.imgs[idx]), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1649866461224,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "PVyl06hUW3xv",
    "outputId": "66b0efdc-9b8b-4d80-f804-9d582fb7825e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 46\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, shuffle=True)\n",
    "loaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(MvTecDataset(X_train, y_train, model_cfg.transform_test), batch_size=batch_size, shuffle=True, drop_last=True),    \n",
    "    \"test\": torch.utils.data.DataLoader(MvTecDataset(X_test, y_test, model_cfg.transform_test), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "}\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1649811146746,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "dYyXdfhk4gI4"
   },
   "outputs": [],
   "source": [
    "# dataset = MvTecDataset(images, labels, model_cfg.transform_test)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "# loaders = {\n",
    "#     \"train\": torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True),    \n",
    "#     \"test\": torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# }\n",
    "# print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6n1DeuaQ6U5"
   },
   "source": [
    "## SWAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1649866461224,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "Ckr3SWsIn5ui"
   },
   "outputs": [],
   "source": [
    "ch_dir='./checkpoints'\n",
    "\n",
    "# no of epochs to run\n",
    "epochs=20\n",
    "# checkpoint save frequency\n",
    "save_freq=20\n",
    "# test set evaluation frequency\n",
    "eval_freq=5\n",
    "# learning rate parameters\n",
    "lr_init=0.1\n",
    "momentum=0.9\n",
    "wd=1e-4\n",
    "\n",
    "# use SWA procedure\n",
    "swa=True\n",
    "# epoch to start saving snapshots for SWAG\n",
    "swa_start=10\n",
    "swa_lr=0.01\n",
    "swa_c_epochs=1\n",
    "\n",
    "# use covariance matrix in SWAG prodecure\n",
    "cov_mat=True\n",
    "# no of models to create\n",
    "max_num_models=20\n",
    "swa_resume=None\n",
    "loss=\"CE\"\n",
    "seed=1\n",
    "resume=None\n",
    "no_schedule=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1649866461225,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "4K1IHXJwJlAR",
    "outputId": "c91048e4-ea1a-4607-cf95-88a44deba49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWAG training\n",
      "<class 'swag.models.preresnet.Bottleneck'>\n"
     ]
    }
   ],
   "source": [
    "if cov_mat:\n",
    "    no_cov_mat = False\n",
    "else:\n",
    "    no_cov_mat = True\n",
    "if swa:\n",
    "    print(\"SWAG training\")\n",
    "    # SWAG initialization\n",
    "    swag_model = SWAG(\n",
    "        model_cfg.base,\n",
    "        no_cov_mat=no_cov_mat,\n",
    "        max_num_models=max_num_models,\n",
    "        *model_cfg.args,\n",
    "        num_classes=num_classes,\n",
    "        **model_cfg.kwargs\n",
    "    )\n",
    "    swag_model.to(device)\n",
    "else:\n",
    "    print(\"SGD training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1649866461225,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "6n_LgWwWKvKR"
   },
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    swa_lr=0.02\n",
    "    lr_init=0.1\n",
    "    swa_start=10\n",
    "    swa=True\n",
    "\n",
    "    t = (epoch) / (swa_start if swa else epochs)\n",
    "    lr_ratio = swa_lr / lr_init if swa else 0.01\n",
    "    if t <= 0.5:\n",
    "        factor = 1.0\n",
    "    elif t <= 0.9:\n",
    "        factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
    "    else:\n",
    "        factor = lr_ratio\n",
    "    return lr_init * factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1649866461226,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "wV_MS1yxKvOC"
   },
   "outputs": [],
   "source": [
    "# use a slightly modified loss function that allows input of model\n",
    "if loss == \"CE\":\n",
    "    criterion = losses.cross_entropy\n",
    "    # criterion = F.cross_entropy\n",
    "elif loss == \"adv_CE\":\n",
    "    criterion = losses.adversarial_cross_entropy\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=lr_init, momentum=momentum, weight_decay=wd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1649866461226,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "P7ujNvz6JlDh"
   },
   "outputs": [],
   "source": [
    "# if resume is not None:\n",
    "#     print(\"Resume training from %s\" % args.resume)\n",
    "#     checkpoint = torch.load(args.resume)\n",
    "#     start_epoch = checkpoint[\"epoch\"]\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "#     optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1649866461226,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "wIBd6ztTMid4"
   },
   "outputs": [],
   "source": [
    "# if swa and swa_resume is not None:\n",
    "#     checkpoint = torch.load(swa_resume)\n",
    "#     swag_model = SWAG(\n",
    "#         model_cfg.base,\n",
    "#         no_cov_mat=no_cov_mat,\n",
    "#         max_num_models=max_num_models,\n",
    "#         loading=True,\n",
    "#         *model_cfg.args,\n",
    "#         num_classes=num_classes,\n",
    "#         **model_cfg.kwargs\n",
    "#     )\n",
    "#     swag_model.to(device)\n",
    "#     swag_model.load_state_dict(checkpoint[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1649866461227,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "UAB4Pop6Migl"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "columns = [\"ep\", \"lr\", \"tr_loss\", \"tr_acc\", \"te_loss\", \"te_acc\", \"time\", \"mem_usage\"]\n",
    "if swa:\n",
    "    columns = columns[:-2] + [\"swa_te_loss\", \"swa_te_acc\"] + columns[-2:]\n",
    "    swag_res = {\"loss\": None, \"accuracy\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1649866461227,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "gP24U7aFMii4"
   },
   "outputs": [],
   "source": [
    "utils.save_checkpoint(\n",
    "    ch_dir,\n",
    "    start_epoch,\n",
    "    state_dict=model.state_dict(),\n",
    "    optimizer=optimizer.state_dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26764,
     "status": "ok",
     "timestamp": 1649866496485,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "uwZHm7aJMiom",
    "outputId": "abadee5d-185f-48e9-d6fb-fae8125bce3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sgd_ens_preds = None\n",
    "sgd_targets = None\n",
    "n_ensembled = 0.0\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # print(epoch)\n",
    "    time_ep = time.time()\n",
    "\n",
    "    if not no_schedule:\n",
    "        lr = schedule(epoch)\n",
    "        utils.adjust_learning_rate(optimizer, lr)\n",
    "    else:\n",
    "        lr = lr_init\n",
    "\n",
    "    if (swa and (epoch + 1) > swa_start) and cov_mat:\n",
    "        # print('in if')\n",
    "        train_res = utils.train_epoch(loaders[\"train\"], model, criterion, optimizer, cuda=use_cuda)\n",
    "    else:\n",
    "        # print('in else')\n",
    "        train_res = utils.train_epoch(loaders[\"train\"], model, criterion, optimizer, cuda=use_cuda)\n",
    "        # print('in else after')\n",
    "\n",
    "\n",
    "    if (\n",
    "        epoch == 0\n",
    "        or epoch % eval_freq == eval_freq - 1\n",
    "        or epoch == epochs - 1\n",
    "    ):  \n",
    "#         print('test_res 312')\n",
    "\n",
    "        test_res = utils.eval(loaders[\"test\"], model, criterion, cuda=use_cuda)\n",
    "    else:\n",
    "        test_res = {\"loss\": None, \"accuracy\": None}\n",
    "#         print('test_res 317')\n",
    "\n",
    "    if (\n",
    "        swa\n",
    "        and (epoch + 1) > swa_start\n",
    "        and (epoch + 1 - swa_start) % swa_c_epochs == 0\n",
    "    ):\n",
    "        # sgd_preds, sgd_targets = utils.predictions(loaders[\"test\"], model)\n",
    "        sgd_res = utils.predict(loaders[\"test\"], model)\n",
    "        sgd_preds = sgd_res[\"predictions\"]\n",
    "        sgd_targets = sgd_res[\"targets\"]\n",
    "        print(\"updating sgd_ens\")\n",
    "        if sgd_ens_preds is None:\n",
    "            sgd_ens_preds = sgd_preds.copy()\n",
    "        else:\n",
    "            # TODO: rewrite in a numerically stable way\n",
    "            sgd_ens_preds = sgd_ens_preds * n_ensembled / (\n",
    "                n_ensembled + 1\n",
    "            ) + sgd_preds / (n_ensembled + 1)\n",
    "        n_ensembled += 1\n",
    "        swag_model.collect_model(model)\n",
    "        if (\n",
    "            epoch == 0\n",
    "            or epoch % eval_freq == eval_freq - 1\n",
    "            or epoch == epochs - 1\n",
    "        ):\n",
    "            swag_model.sample(0.0)\n",
    "            utils.bn_update(loaders[\"train\"], swag_model)\n",
    "            swag_res = utils.eval(loaders[\"test\"], swag_model, criterion)\n",
    "        else:\n",
    "            swag_res = {\"loss\": None, \"accuracy\": None}\n",
    "\n",
    "    if (epoch + 1) % save_freq == 0:\n",
    "        utils.save_checkpoint(\n",
    "            ch_dir,\n",
    "            epoch + 1,\n",
    "            state_dict=model.state_dict(),\n",
    "            optimizer=optimizer.state_dict(),\n",
    "        )\n",
    "        if swa:\n",
    "            # Save SWAG Weights\n",
    "            utils.save_checkpoint(\n",
    "                ch_dir, epoch + 1, name=\"swag\", state_dict=swag_model.state_dict()\n",
    "            )\n",
    "\n",
    "    time_ep = time.time() - time_ep\n",
    "    \n",
    "    if use_cuda:\n",
    "        memory_usage = torch.cuda.memory_allocated() / (1024.0 ** 3)\n",
    "    else:\n",
    "        memory_usage = None \n",
    "    values = [\n",
    "        epoch + 1,\n",
    "        lr,\n",
    "        train_res[\"loss\"],\n",
    "        train_res[\"accuracy\"],\n",
    "        test_res[\"loss\"],\n",
    "        test_res[\"accuracy\"],\n",
    "        time_ep,\n",
    "        memory_usage,\n",
    "    ]\n",
    "    if swa:\n",
    "        values = values[:-2] + [swag_res[\"loss\"], swag_res[\"accuracy\"]] + values[-2:]\n",
    "    table = tabulate.tabulate([values], columns, tablefmt=\"simple\", floatfmt=\"8.4f\")\n",
    "    if epoch % 40 == 0:\n",
    "        table = table.split(\"\\n\")\n",
    "        table = \"\\n\".join([table[1]] + table)\n",
    "    else:\n",
    "        table = table.split(\"\\n\")[2]\n",
    "    print(table)\n",
    "\n",
    "if epochs % save_freq != 0:\n",
    "#     Checkpoint save\n",
    "    utils.save_checkpoint(\n",
    "        ch_dir,\n",
    "        epochs,\n",
    "        state_dict=model.state_dict(),\n",
    "        optimizer=optimizer.state_dict(),\n",
    "    )\n",
    "    if swa and epochs > swa_start:\n",
    "        utils.save_checkpoint(\n",
    "            a, epochs, name=\"swag\", state_dict=swag_model.state_dict()\n",
    "        )\n",
    "\n",
    "if swa:\n",
    "    np.savez(\n",
    "        os.path.join(ch_dir, \"sgd_ens_preds.npz\"),\n",
    "        predictions=sgd_ens_preds,\n",
    "        targets=sgd_targets,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5724,
     "status": "ok",
     "timestamp": 1649868296880,
     "user": {
      "displayName": "Muhammad Ammar Ahmed",
      "userId": "06626931976988068439"
     },
     "user_tz": -120
    },
    "id": "Ou1n_YWcW8kL",
    "outputId": "312daefe-889e-47df-abee-775f212243b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swag_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1Avt8w9LMAJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "temirlan_mvtec_swag.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
