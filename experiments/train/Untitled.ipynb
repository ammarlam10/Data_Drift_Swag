{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import tabulate\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Z0049YKN\\Desktop\\LMU\\3rd sem\\uncertinitz quantification in deep learning\\swa_gaussian-master\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swag import data, models, utils, losses\n",
    "from swag.posteriors import SWAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swag import data, models, utils, losses\n",
    "from swag.posteriors import SWAG\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"SGD/SWA training\")\n",
    "parser.add_argument(\n",
    "    \"--dir\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=True,\n",
    "    help=\"training directory (default: None)\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--dataset\", type=str, default=\"CIFAR10\", help=\"dataset name (default: CIFAR10)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=True,\n",
    "    metavar=\"PATH\",\n",
    "    help=\"path to datasets location (default: None)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_test\",\n",
    "    dest=\"use_test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"use test dataset instead of validation (default: False)\",\n",
    ")\n",
    "parser.add_argument(\"--split_classes\", type=int, default=None)\n",
    "parser.add_argument(\n",
    "    \"--batch_size\",\n",
    "    type=int,\n",
    "    default=128,\n",
    "    metavar=\"N\",\n",
    "    help=\"input batch size (default: 128)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    metavar=\"N\",\n",
    "    help=\"number of workers (default: 4)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=True,\n",
    "    metavar=\"MODEL\",\n",
    "    help=\"model name (default: None)\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--resume\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    metavar=\"CKPT\",\n",
    "    help=\"checkpoint to resume training from (default: None)\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    type=int,\n",
    "    default=200,\n",
    "    metavar=\"N\",\n",
    "    help=\"number of epochs to train (default: 200)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_freq\",\n",
    "    type=int,\n",
    "    default=25,\n",
    "    metavar=\"N\",\n",
    "    help=\"save frequency (default: 25)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_freq\",\n",
    "    type=int,\n",
    "    default=5,\n",
    "    metavar=\"N\",\n",
    "    help=\"evaluation frequency (default: 5)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_init\",\n",
    "    type=float,\n",
    "    default=0.01,\n",
    "    metavar=\"LR\",\n",
    "    help=\"initial learning rate (default: 0.01)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--momentum\",\n",
    "    type=float,\n",
    "    default=0.9,\n",
    "    metavar=\"M\",\n",
    "    help=\"SGD momentum (default: 0.9)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wd\", type=float, default=1e-4, help=\"weight decay (default: 1e-4)\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--swa\", action=\"store_true\", help=\"swa usage flag (default: off)\")\n",
    "parser.add_argument(\n",
    "    \"--swa_start\",\n",
    "    type=float,\n",
    "    default=161,\n",
    "    metavar=\"N\",\n",
    "    help=\"SWA start epoch number (default: 161)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--swa_lr\", type=float, default=0.02, metavar=\"LR\", help=\"SWA LR (default: 0.02)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--swa_c_epochs\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    metavar=\"N\",\n",
    "    help=\"SWA model collection frequency/cycle length in epochs (default: 1)\",\n",
    ")\n",
    "parser.add_argument(\"--cov_mat\", action=\"store_true\", help=\"save sample covariance\")\n",
    "parser.add_argument(\n",
    "    \"--max_num_models\",\n",
    "    type=int,\n",
    "    default=20,\n",
    "    help=\"maximum number of SWAG models to save\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--swa_resume\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    metavar=\"CKPT\",\n",
    "    help=\"checkpoint to restor SWA from (default: None)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loss\",\n",
    "    type=str,\n",
    "    default=\"CE\",\n",
    "    help=\"loss to use for training model (default: Cross-entropy)\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\"\n",
    ")\n",
    "parser.add_argument(\"--no_schedule\", action=\"store_true\", help=\"store schedule\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.device = None\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    args.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    args.device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Preparing directory %s\" % args.dir)\n",
    "os.makedirs(args.dir, exist_ok=True)\n",
    "with open(os.path.join(args.dir, \"command.sh\"), \"w\") as f:\n",
    "    f.write(\" \".join(sys.argv))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print(\"Using model %s\" % args.model)\n",
    "model_cfg = getattr(models, args.model)\n",
    "\n",
    "print(\"Loading dataset %s from %s\" % (args.dataset, args.data_path))\n",
    "loaders, num_classes = data.loaders(\n",
    "    args.dataset,\n",
    "    args.data_path,\n",
    "    args.batch_size,\n",
    "    args.num_workers,\n",
    "    model_cfg.transform_train,\n",
    "    model_cfg.transform_test,\n",
    "    use_validation=not args.use_test,\n",
    "    split_classes=args.split_classes,\n",
    ")\n",
    "\n",
    "print(\"Preparing model\")\n",
    "print(*model_cfg.args)\n",
    "model = model_cfg.base(*model_cfg.args, num_classes=num_classes, **model_cfg.kwargs)\n",
    "model.to(args.device)\n",
    "\n",
    "\n",
    "if args.cov_mat:\n",
    "    args.no_cov_mat = False\n",
    "else:\n",
    "    args.no_cov_mat = True\n",
    "if args.swa:\n",
    "    print(\"SWAG training\")\n",
    "    swag_model = SWAG(\n",
    "        model_cfg.base,\n",
    "        no_cov_mat=args.no_cov_mat,\n",
    "        max_num_models=args.max_num_models,\n",
    "        *model_cfg.args,\n",
    "        num_classes=num_classes,\n",
    "        **model_cfg.kwargs\n",
    "    )\n",
    "    swag_model.to(args.device)\n",
    "else:\n",
    "    print(\"SGD training\")\n",
    "\n",
    "\n",
    "def schedule(epoch):\n",
    "    t = (epoch) / (args.swa_start if args.swa else args.epochs)\n",
    "    lr_ratio = args.swa_lr / args.lr_init if args.swa else 0.01\n",
    "    if t <= 0.5:\n",
    "        factor = 1.0\n",
    "    elif t <= 0.9:\n",
    "        factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
    "    else:\n",
    "        factor = lr_ratio\n",
    "    return args.lr_init * factor\n",
    "\n",
    "\n",
    "# use a slightly modified loss function that allows input of model\n",
    "if args.loss == \"CE\":\n",
    "    criterion = losses.cross_entropy\n",
    "    # criterion = F.cross_entropy\n",
    "elif args.loss == \"adv_CE\":\n",
    "    criterion = losses.adversarial_cross_entropy\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=args.lr_init, momentum=args.momentum, weight_decay=args.wd\n",
    ")\n",
    "\n",
    "start_epoch = 0\n",
    "if args.resume is not None:\n",
    "    print(\"Resume training from %s\" % args.resume)\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "if args.swa and args.swa_resume is not None:\n",
    "    checkpoint = torch.load(args.swa_resume)\n",
    "    swag_model = SWAG(\n",
    "        model_cfg.base,\n",
    "        no_cov_mat=args.no_cov_mat,\n",
    "        max_num_models=args.max_num_models,\n",
    "        loading=True,\n",
    "        *model_cfg.args,\n",
    "        num_classes=num_classes,\n",
    "        **model_cfg.kwargs\n",
    "    )\n",
    "    swag_model.to(args.device)\n",
    "    swag_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "columns = [\"ep\", \"lr\", \"tr_loss\", \"tr_acc\", \"te_loss\", \"te_acc\", \"time\", \"mem_usage\"]\n",
    "if args.swa:\n",
    "    columns = columns[:-2] + [\"swa_te_loss\", \"swa_te_acc\"] + columns[-2:]\n",
    "    swag_res = {\"loss\": None, \"accuracy\": None}\n",
    "\n",
    "utils.save_checkpoint(\n",
    "    args.dir,\n",
    "    start_epoch,\n",
    "    state_dict=model.state_dict(),\n",
    "    optimizer=optimizer.state_dict(),\n",
    ")\n",
    "\n",
    "sgd_ens_preds = None\n",
    "sgd_targets = None\n",
    "n_ensembled = 0.0\n",
    "\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    time_ep = time.time()\n",
    "\n",
    "    if not args.no_schedule:\n",
    "        lr = schedule(epoch)\n",
    "        utils.adjust_learning_rate(optimizer, lr)\n",
    "    else:\n",
    "        lr = args.lr_init\n",
    "\n",
    "    if (args.swa and (epoch + 1) > args.swa_start) and args.cov_mat:\n",
    "        train_res = utils.train_epoch(loaders[\"train\"], model, criterion, optimizer, cuda=use_cuda)\n",
    "    else:\n",
    "        train_res = utils.train_epoch(loaders[\"train\"], model, criterion, optimizer, cuda=use_cuda)\n",
    "\n",
    "    if (\n",
    "        epoch == 0\n",
    "        or epoch % args.eval_freq == args.eval_freq - 1\n",
    "        or epoch == args.epochs - 1\n",
    "    ):\n",
    "        test_res = utils.eval(loaders[\"test\"], model, criterion, cuda=use_cuda)\n",
    "    else:\n",
    "        test_res = {\"loss\": None, \"accuracy\": None}\n",
    "\n",
    "    if (\n",
    "        args.swa\n",
    "        and (epoch + 1) > args.swa_start\n",
    "        and (epoch + 1 - args.swa_start) % args.swa_c_epochs == 0\n",
    "    ):\n",
    "        # sgd_preds, sgd_targets = utils.predictions(loaders[\"test\"], model)\n",
    "        sgd_res = utils.predict(loaders[\"test\"], model)\n",
    "        sgd_preds = sgd_res[\"predictions\"]\n",
    "        sgd_targets = sgd_res[\"targets\"]\n",
    "        print(\"updating sgd_ens\")\n",
    "        if sgd_ens_preds is None:\n",
    "            sgd_ens_preds = sgd_preds.copy()\n",
    "        else:\n",
    "            # TODO: rewrite in a numerically stable way\n",
    "            sgd_ens_preds = sgd_ens_preds * n_ensembled / (\n",
    "                n_ensembled + 1\n",
    "            ) + sgd_preds / (n_ensembled + 1)\n",
    "        n_ensembled += 1\n",
    "        swag_model.collect_model(model)\n",
    "        if (\n",
    "            epoch == 0\n",
    "            or epoch % args.eval_freq == args.eval_freq - 1\n",
    "            or epoch == args.epochs - 1\n",
    "        ):\n",
    "            swag_model.sample(0.0)\n",
    "            utils.bn_update(loaders[\"train\"], swag_model)\n",
    "            swag_res = utils.eval(loaders[\"test\"], swag_model, criterion)\n",
    "        else:\n",
    "            swag_res = {\"loss\": None, \"accuracy\": None}\n",
    "\n",
    "    if (epoch + 1) % args.save_freq == 0:\n",
    "        utils.save_checkpoint(\n",
    "            args.dir,\n",
    "            epoch + 1,\n",
    "            state_dict=model.state_dict(),\n",
    "            optimizer=optimizer.state_dict(),\n",
    "        )\n",
    "        if args.swa:\n",
    "            utils.save_checkpoint(\n",
    "                args.dir, epoch + 1, name=\"swag\", state_dict=swag_model.state_dict()\n",
    "            )\n",
    "\n",
    "    time_ep = time.time() - time_ep\n",
    "    \n",
    "    if use_cuda:\n",
    "        memory_usage = torch.cuda.memory_allocated() / (1024.0 ** 3)\n",
    "        \n",
    "    values = [\n",
    "        epoch + 1,\n",
    "        lr,\n",
    "        train_res[\"loss\"],\n",
    "        train_res[\"accuracy\"],\n",
    "        test_res[\"loss\"],\n",
    "        test_res[\"accuracy\"],\n",
    "        time_ep,\n",
    "        memory_usage,\n",
    "    ]\n",
    "    if args.swa:\n",
    "        values = values[:-2] + [swag_res[\"loss\"], swag_res[\"accuracy\"]] + values[-2:]\n",
    "    table = tabulate.tabulate([values], columns, tablefmt=\"simple\", floatfmt=\"8.4f\")\n",
    "    if epoch % 40 == 0:\n",
    "        table = table.split(\"\\n\")\n",
    "        table = \"\\n\".join([table[1]] + table)\n",
    "    else:\n",
    "        table = table.split(\"\\n\")[2]\n",
    "    print(table)\n",
    "\n",
    "if args.epochs % args.save_freq != 0:\n",
    "    utils.save_checkpoint(\n",
    "        args.dir,\n",
    "        args.epochs,\n",
    "        state_dict=model.state_dict(),\n",
    "        optimizer=optimizer.state_dict(),\n",
    "    )\n",
    "    if args.swa and args.epochs > args.swa_start:\n",
    "        utils.save_checkpoint(\n",
    "            args.dir, args.epochs, name=\"swag\", state_dict=swag_model.state_dict()\n",
    "        )\n",
    "\n",
    "if args.swa:\n",
    "    np.savez(\n",
    "        os.path.join(args.dir, \"sgd_ens_preds.npz\"),\n",
    "        predictions=sgd_ens_preds,\n",
    "        targets=sgd_targets,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
